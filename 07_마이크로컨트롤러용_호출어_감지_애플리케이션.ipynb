{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07.마이크로컨트롤러용 호출어 감지 애플리케이션.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "aCKti8ST_dgA",
        "gGNmE9jk_l7r",
        "TFEbGiww_r9y",
        "9sJBoqisGzA1",
        "-8DhBEhmVdJ6",
        "I7dGm7hWXRS4"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCKti8ST_dgA"
      },
      "source": [
        "# **지능형 IoT 응용** (2021년 2학기)\n",
        "#### 한국기술교육대학교 컴퓨터공학부 스마트 IoT 트랙\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv7ZFfpv_ftn"
      },
      "source": [
        "# 07. 마이크로컨트롤러용 호출어 감지 애플리케이션\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGNmE9jk_l7r"
      },
      "source": [
        "---\n",
        "\n",
        "### Acknowledgement\n",
        "\n",
        "\n",
        "이 자료는 다음 서적의 내용을 바탕으로 작성되었음\n",
        "- 초소형 머신러닝 TinyML, 피트 워든, 대니얼 시투나야케 지음, 맹윤호, 임지순 옮김, 한빛미디어\n",
        " - 7장\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFEbGiww_r9y"
      },
      "source": [
        "### 왜 호출어 감지 애플리케이션이 필요할까?\n",
        "\n",
        "- 그 동안 여러 가지 음성 인식 비서 제품이 등장했음\n",
        " - Google Assistant (https://assistant.google.com/)\n",
        " - Apple Siri (https://www.apple.com/kr/siri/)\n",
        " - Amazon Alexa (https://developer.amazon.com/en-US/alexa)\n",
        " - Samsung Bixby (https://www.samsung.com/sec/apps/bixby/)\n",
        " - SKT NUGU (https://www.nugu.co.kr/)\n",
        "\n",
        "- 음성 인식 서비스는 대부분의 스마트폰에 내장되어 있고, 다양한 스마트 스피커 제품으로도 나와 있음\n",
        "\n",
        "- 이러한 음성 인식 서비스는 대규모의 머신러닝 모델을 실행하는 서버에 의해 구동됨\n",
        " - 서버에서 음성 인식, 자연어 처리, 사용자 쿼리에 대한 응답 생성을 수행\n",
        " - 사용자가 질문을 하면 오디오 스트림으로 서버에 전송되고 서버는 의미를 파악하여 필요한 정보를 찾은 다음 적절한 응답을 보내는 것\n",
        "\n",
        "- 음성 인식 비서는 사용자가 언제 어디에 있든 24시간 내내 목소리를 들을 수 있어야 함\n",
        " - 사용자가 거실에 앉아 있을 때나\n",
        " - 차를 타고 고속도로를 주행할 때나\n",
        " - 스마트폰을 들고 야외에서 대화를 할 때나\n",
        "\n",
        "- 그런데 사용자가 언제 어디에서든 하는 말을 다 듣고 서버로 전송한다면?\n",
        " - 심각한 프라이버시 문제 발생 가능\n",
        " - 엄청난 네트워크 대역폭 사용\n",
        " - 막대한 연산량, 에너지 사용 (스마트폰 같은 장치의 배터리 급격한 소모)\n",
        "\n",
        "- 음성 인식 비서가 듣고 처리해야 하는 말을 구별해주기 위해서 호출어(Hotword, Wake word, Wake-up command) 감지 방법을 사용\n",
        " - \"Hey Google\" / \"OK Google\"\n",
        " - \"Hey Siri\"\n",
        " - \"Alexa\"\n",
        " - \"Hi Bixby\"\n",
        " - \"아리아\" (NUGU)\n",
        "\n",
        "- 호출어를 인식하는 작은 모델을 훈련시켜 저전력 칩에서 실행할 수 있으며 이를 스마트폰이나 스마트 스피커 같은 디바이스에 내장하면 항상 호출어를 들을 수 있음\n",
        " - TinML이 중요한 역할을 할 수 있음\n",
        " - 불필요한 데이터를 서버에 전송하지 않고 개인정보보호, 효율성을 향상할 수 있는 방법  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sJBoqisGzA1"
      },
      "source": [
        "## 7.1 호출어 감지 애플리케이션\n",
        "\n",
        "- 음성 오디오를 분류하는 임베디드 애플리케이션\n",
        " - 음성 명령 데이터셋을 가지고 훈련된 18KB 모델을 사용\n",
        " - 'yes', 'no' 두 단어를 인식하고 알 수 없는 단어, 무음을 구별함\n",
        " - 마이크를 이용해 주변에서 발생하는 오디오를 듣고 인식 결과에 따라 LED를 켜거나 화면에 데이터를 표시\n",
        "\n",
        "- 애플리케이션 예제 코드\n",
        " - TFLM github 저장소 경로: tensorflow/lite/micro/examples/micro_speech/\n",
        " - https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples/micro_speech \n",
        "\n",
        "\n",
        "앞서 보았던 머신러닝 애플리케이션의 일반적인 작동 방식과 비슷한 흐름으로 동작하지만 호출어 감지 애플리케이션은 더 복잡함\n",
        "\n",
        "- 머신러닝 애플리케이션의 일반적인 작업 흐름\n",
        " - 입력을 얻는다\n",
        " - 입력을 전처리해 모델에 공급하기 적합한 특징을 추출한다\n",
        " - 처리된 입력에 대한 추론을 실행한다\n",
        " - 모델의 출력을 후처리한다\n",
        " - 결과 정보를 사용하여 필요한 작업을 수행한다\n",
        "\n",
        "- 이전에 본 hello world 예제는 각 단계가 매우 간단\n",
        " - 입력은 단순한 부동소수점 숫자\n",
        " - 특별한 전처리가 없고 특징 추출 단계가 없음\n",
        " - 특별한 후처리 없음\n",
        "\n",
        "- 호출어 감지 애플리케이션이 복잡한 이유\n",
        " - 마이크에서 오디오 데이터를 입력으로 받음\n",
        " - 모델에 공급되기 전에 많은 전처리 필요\n",
        " - 모델은 분류기의 일종으로 각 클래스에 속할 확률을 출력하는데 이 결과를 파싱하고 이해 가능하도록 가공해야 함\n",
        " - 실시간 입력되는 라이브 데이터에 대해 지속적으로 추론을 수행해야 함. 추론의 흐름을 이해하도록 코드를 작성해야 함\n",
        " - 모델이 더 크고 복잡함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wyku8UOPJGqq"
      },
      "source": [
        "## 7.2 애플리케이션 아키텍처\n",
        "\n",
        "<img src=\"./07.호출어_감지_애플리케이션/07-1.architecture.png\" width=\"90%\" height=\"90%\">\n",
        "<center>호출어 감지 애플리케이션 아키텍처</center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8DhBEhmVdJ6"
      },
      "source": [
        "#### 애플리케이션의 구성 요소\n",
        "\n",
        "- 메인 루프\n",
        " - hello world 예제와 마찬가지로 호출어 감지 애플리케이션도 연속 루프로 실행됨\n",
        " - 모든 후속 프로세스는 루프 안에 포함되어 있으며 마이크로컨트롤러가 실행할 수 있는 속도로 계속 실행됨\n",
        "\n",
        "- 오디오 추출기\n",
        " - 마이크에서 raw 오디오 데이터를 캡처함\n",
        " - 오디오 캡처 방법은 장치마다 다르기 때문에 이 모듈은 장치에 맞는 커스텀 버전으로 재정의할 수 있음\n",
        "\n",
        "- 특징 추출기\n",
        " - raw 오디오 데이터를 모델에 필요한 스펙트로그램(spectrogram) 형식으로 변환함\n",
        " - 메인 루프의 일부로 인터프리터에 1초 간격의 오버랩이 있는 데이터 시퀀스를 제공함\n",
        "\n",
        "- TF Lite 인터프리터\n",
        " - 텐서플로우 라이트 모델을 실행하여 입력 스펙트로그램을 확률 세트로 변환함\n",
        " - 여기서 확률 세트라는 것은 각 클래스(yes, no, 알 수 없음, 무음) 별 확률값의 모음을 의미\n",
        "\n",
        "- 모델\n",
        " - 모델은 데이터 배열로 표현되며 인터프리터에 의해 실행됨\n",
        "\n",
        "- 명령 인식기\n",
        " - 인터프리터의 출력 결과를 이용하여 호출어 음성을 인식함\n",
        " - 추론은 초당 여러 번 실행되므로 여러 개의 결과를 집계하고 알려진 단어(yes/no)가 들렸는지 평균적으로 결정함\n",
        "\n",
        "- 명령 응답기\n",
        " - 음성 명령(호출어)이 인식되면 장치의 출력 기능을 통해 사용자에게 알림\n",
        "  - 장치에 따라 LED를 깜박이거나 디스플레이에 데이터를 표시할 수 있음\n",
        "  - 장치 유형에 따라 이 모듈을 재정의할 수 있음\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7dGm7hWXRS4"
      },
      "source": [
        "\n",
        "#### 예제 모델 소개\n",
        "\n",
        "- 모델 훈련 데이터셋 (Speech Commands)\n",
        " - 온라인 크라우드소싱 방식으로 수집된 30여개의 짧은 단어(1초 길이) 오디오 데이터 묶음으로 구성됨\n",
        " - 버전 1 (약 65000개), 버전 2 (약 106000개) \n",
        " - https://www.tensorflow.org/datasets/catalog/speech_commands\n",
        " - 관련 arXiv 논문 링크: https://arxiv.org/abs/1804.03209\n",
        "\n",
        "- 7장 예제 모델의 출력\n",
        " - yes, no, unknown(알 수 없음), silence(무음) 네 가지 클래스를 분류할 수 있게 훈련됨\n",
        " - 1초의 오디오 데이터를 사용하여 네 개 클래스 중 하나를 나타낼 가능성을 예측하여 각 클래스에 하나씩 네 개의 확률을 출력함\n",
        " - 위 데이터셋을 사용하여 다른 단어를 인식하도록 모델을 훈련할 수도 있음 (8장에서 소개)\n",
        "\n",
        "- 모델의 입력\n",
        " - 오디오 데이터의 Spectrogram(스펙트로그램)\n",
        " - raw 오디오 데이터를 전처리하여 spectrogram을 만든 후 이를 모델의 입력으로 사용\n",
        " - spectrogram 데이터는 2차원 배열로 나타낼 수 있기 때문에 2D 텐서(tensor)로 모델에 공급됨\n",
        " \n",
        "- Spectrogram \n",
        " - 소리나 파동을 시각화하여 파악하기 위한 기법으로 파형(waveform)과 스펙트럼(spectrum)의 특징이 조합되어 있는 heat map 그래프로 표현됨\n",
        " - 파형(waveform)에서는 시간 축의 변화에 따른 진폭의 변화를 볼 수 있고 스펙트럼(spectrum)에서는 주파수 축의 변화에 따른 진폭의 변화를 볼 수 있는데, 스펙트로그램에서는 시간 축과 주파수 축의 변화에 따른 진폭의 차이를 한번에 확인할 수 있음\n",
        " - 위키백과 내용 참고 (https://ko.wikipedia.org/wiki/%EC%8A%A4%ED%8E%99%ED%8A%B8%EB%A1%9C%EA%B7%B8%EB%9E%A8)\n",
        "\n",
        "- 스펙트로그램 예제\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/c/c5/Spectrogram-19thC.png\">\n",
        "<center>(이미지 출처: https://en.wikipedia.org/wiki/Spectrogram)</center>\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1225/0*2xSeYn8Hghl8MTar.png\">\n",
        "<center>(이미지 출처: https://medium.com/@ODSC/deep-learning-for-speech-recognition-cbbebab15f0d)</center>\n",
        "\n",
        "\n",
        "- 모델 신경망 아키텍처\n",
        " - CNN(Convolutional Neural Network)\n",
        " - CNN은 인접한 값 그룹 사이의 관계 정보가 포함된 다차원 텐서에 잘 작동하도록 설계된 네트워크로서 인접한 픽셀 그룹이 모양, 패턴, 질감을 나타낼 수 있는 이미지 데이터에 많이 사용\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lf_Sj4C1CbdR"
      },
      "source": [
        "## 7.3 테스트 코드\n",
        "\n",
        "#### 주요 코드\n",
        "- micro_speech_test.cc : 입력 오디오 신호의 스펙트로그램 데이터에 대한 추론을 실행하고 결과를 해석하는 방법\n",
        "- audio_provider_test.cc : 오디오 추출기를 사용하는 방법\n",
        "- feature_provider_mock_test.cc : 데이터를 전달하기 위한 오디오 추출기의 모의(가짜) 구현으로 특징 추출기를 사용하는 방법\n",
        "- recognize_commands_test.cc : 호출어 인식 여부를 결정하기 위해 모델 출력을 해석하는 방법\n",
        "- command_responder_test.cc : 명령 응답기를 호출하여 출력을 트리거하는 방법\n",
        "\n",
        "##### [github 저장소] https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples/micro_speech\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvtOg_-KV991"
      },
      "source": [
        "### 7.3.1 기본 흐름 (micro_speech_test.cc)\n",
        "\n",
        "- micro_speech_test.cc에서 모델 로드, 인터프리터 설정, 텐서 할당, 인터프리터 실행 등의 기본 동작은 이전 hello world 예제와 비슷함\n",
        " - https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech/micro_speech_test.cc\n",
        "- 주요 차이점\n",
        " - hello world 예제에서는 AllOpsResolver를 이용하여 모델을 실행하는 데 필요한 Op를 가져옴. 하지만 이 방법은 사용 가능한 모든 Op를 로드하게 되어 사용하지 않는 Op에 대한 메모리 자원이 낭비되는 단점이 있음\n",
        " - 따라서 모델 실행에 필요한 Op만 포함하도록 하기 위해 MicroMutableOpResolver를 사용\n",
        " - 스펙트로그램 데이터를 이용하여 호출어를 인식하는데 CNN 모델을 사용하므로 CNN 모델 실행에 필요한 Op(연산)만 로드하도록 되어 있음\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJmn196aYze5"
      },
      "source": [
        "```cpp\n",
        "#include \"tensorflow/lite/micro/examples/micro_speech/micro_features/no_micro_features_data.h\"\n",
        "#include \"tensorflow/lite/micro/examples/micro_speech/micro_features/yes_micro_features_data.h\"\n",
        "#include \"tensorflow/lite/micro/examples/micro_speech/micro_speech_model_data.h\"\n",
        "#include \"tensorflow/lite/micro/micro_error_reporter.h\"\n",
        "#include \"tensorflow/lite/micro/micro_interpreter.h\"\n",
        "#include \"tensorflow/lite/micro/micro_mutable_op_resolver.h\"\n",
        "#include \"tensorflow/lite/micro/testing/micro_test.h\"\n",
        "#include \"tensorflow/lite/schema/schema_generated.h\"\n",
        "```\n",
        "\n",
        "- tensorflow/lite/micro/examples/micro_speech/micro_features/no_micro_features_data.h\n",
        " - 테스트 용으로 사용하는 \"no\" 오디오 신호의 특징 데이터\n",
        "- tensorflow/lite/micro/examples/micro_speech/micro_features/yes_micro_features_data.h\n",
        " - 테스트 용으로 사용하는 \"yes\" 오디오 신호의 특징 데이터\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H7x8zIxZe8G"
      },
      "source": [
        "```cpp\n",
        "  // Set up logging.\n",
        "  tflite::MicroErrorReporter micro_error_reporter;\n",
        "\n",
        "  // Map the model into a usable data structure. This doesn't involve any\n",
        "  // copying or parsing, it's a very lightweight operation.\n",
        "  const tflite::Model* model = ::tflite::GetModel(g_micro_speech_model_data);\n",
        "  if (model->version() != TFLITE_SCHEMA_VERSION) {\n",
        "    TF_LITE_REPORT_ERROR(&micro_error_reporter,\n",
        "                         \"Model provided is schema version %d not equal \"\n",
        "                         \"to supported version %d.\\n\",\n",
        "                         model->version(), TFLITE_SCHEMA_VERSION);\n",
        "  }\n",
        "```\n",
        "\n",
        "- 로깅을 위한 MicroErrorReporter 객체 생성\n",
        "- Model 객체 생성\n",
        " - 모델 데이터로는 micro_speech_model_data.h에 정의된 g_micro_speech_model_data 배열 이용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rMRntvraA8I"
      },
      "source": [
        "```cpp\n",
        "  // tflite::AllOpsResolver resolver;\n",
        "  tflite::MicroMutableOpResolver<4> micro_op_resolver;\n",
        "  micro_op_resolver.AddDepthwiseConv2D();\n",
        "  micro_op_resolver.AddFullyConnected();\n",
        "  micro_op_resolver.AddReshape();\n",
        "  micro_op_resolver.AddSoftmax();\n",
        "```\n",
        "\n",
        "- AllOpsResolver 객체 대신 MicroMutableOpResolver 사용\n",
        " - 모델 실행에 필요한 operation만 추가해 줌\n",
        " - 2D convolution 연산을 위한 Op\n",
        " - Fully connected 연산을 위한 Op\n",
        " - 데이터 형상을 변경하기 위한 Op\n",
        " - Softmax 연산을 위한 Op"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrExlk1fcwUb"
      },
      "source": [
        "```cpp\n",
        "  // Create an area of memory to use for input, output, and intermediate arrays.\n",
        "#if defined(XTENSA)\n",
        "  constexpr int tensor_arena_size = 15 * 1024;\n",
        "#else\n",
        "  constexpr int tensor_arena_size = 10 * 1024;\n",
        "#endif\n",
        "  uint8_t tensor_arena[tensor_arena_size];\n",
        "```\n",
        "\n",
        "- Tensor Arena 생성\n",
        " - 입력, 출력, 중간 생성 데이터 텐서를 저장할 메모리 공간"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zov1BElXdHI9"
      },
      "source": [
        "```cpp\n",
        "  // Build an interpreter to run the model with.\n",
        "  tflite::MicroInterpreter interpreter(model, micro_op_resolver, tensor_arena,\n",
        "                                       tensor_arena_size,\n",
        "                                       &micro_error_reporter);\n",
        "  interpreter.AllocateTensors();\n",
        "```\n",
        "\n",
        "- 모델 실행을 위한 인터프리터 객체 생성\n",
        "- 작업 메모리 할당"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBklI6UxdQQ-"
      },
      "source": [
        "```cpp\n",
        "  // Get information about the memory area to use for the model's input.\n",
        "  TfLiteTensor* input = interpreter.input(0);\n",
        "\n",
        "  // Make sure the input has the properties we expect.\n",
        "  TF_LITE_MICRO_EXPECT_NE(nullptr, input);\n",
        "  TF_LITE_MICRO_EXPECT_EQ(2, input->dims->size);\n",
        "  TF_LITE_MICRO_EXPECT_EQ(1, input->dims->data[0]);\n",
        "  TF_LITE_MICRO_EXPECT_EQ(1960, input->dims->data[1]);\n",
        "  TF_LITE_MICRO_EXPECT_EQ(kTfLiteInt8, input->type);\n",
        "```\n",
        "\n",
        "- 입력 데이터에 대한 포인터를 이용해 입력 텐서가 올바른 형태인지 확인\n",
        " - null 포인터가 아니어야 함\n",
        " - 입력 텐서의 차원은 2\n",
        " - 첫번째 차원 (data[0])은 단일 요소를 포함하는 래퍼\n",
        " - 두번재 차원 (data[1])은 스펙트로그램의 데이터를 저장 (49행 40열 데이터 - 1960개 원소)\n",
        " - 데이터 타입은 8비트 integer (kTfLiteInt8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaIanBPkgF3q"
      },
      "source": [
        "```cpp\n",
        "  // Copy a spectrogram created from a .wav audio file of someone saying \"Yes\",\n",
        "  // into the memory area used for the input.\n",
        "  const int8_t* yes_features_data = g_yes_micro_f2e59fea_nohash_1_data;\n",
        "  for (size_t i = 0; i < input->bytes; ++i) {\n",
        "    input->data.int8[i] = yes_features_data[i];\n",
        "  }\n",
        "```\n",
        "\n",
        "- 테스트 추론을 위해 \"yes\"라고 말하는 오디오 신호에서 추출한 특징 데이터를 입력 텐서에 복사\n",
        " - micro_features/yes_micro_features_data.cc에 정의된 배열인 g_yes_micro_f2e59fea_nohash_1_data 이용\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4oOgvJsg981"
      },
      "source": [
        "```cpp\n",
        "  // Run the model on this input and make sure it succeeds.\n",
        "  TfLiteStatus invoke_status = interpreter.Invoke();\n",
        "  if (invoke_status != kTfLiteOk) {\n",
        "    TF_LITE_REPORT_ERROR(&micro_error_reporter, \"Invoke failed\\n\");\n",
        "  }\n",
        "  TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, invoke_status);\n",
        "```\n",
        "\n",
        "- 인터프리터를 실행하고 에러가 없는지 확인"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTSYE4jYhFat"
      },
      "source": [
        "```cpp\n",
        "  // Get the output from the model, and make sure it's the expected size and\n",
        "  // type.\n",
        "  TfLiteTensor* output = interpreter.output(0);\n",
        "  TF_LITE_MICRO_EXPECT_EQ(2, output->dims->size);\n",
        "  TF_LITE_MICRO_EXPECT_EQ(1, output->dims->data[0]);\n",
        "  TF_LITE_MICRO_EXPECT_EQ(4, output->dims->data[1]);\n",
        "  TF_LITE_MICRO_EXPECT_EQ(kTfLiteInt8, output->type);\n",
        "```\n",
        "\n",
        "- 출력 텐서가 올바른 형태인지 확인\n",
        " - 출력 텐서의 차원은 2\n",
        " - 첫번째 차원 (data[0])은 단일 요소를 포함하는 래퍼\n",
        " - 두번재 차원 (data[1])은 모델 추론 결과를 저장 : 4가지 가능한 클래스(yes, no, unknown, silence) 각각의 확률\n",
        " - 데이터 타입은 8비트 integer\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZQYHx3ahymn"
      },
      "source": [
        "```cpp\n",
        "  // There are four possible classes in the output, each with a score.\n",
        "  const int kSilenceIndex = 0;\n",
        "  const int kUnknownIndex = 1;\n",
        "  const int kYesIndex = 2;\n",
        "  const int kNoIndex = 3;\n",
        "\n",
        "  // Make sure that the expected \"Yes\" score is higher than the other classes.\n",
        "  uint8_t silence_score = output->data.int8[kSilenceIndex] + 128;\n",
        "  uint8_t unknown_score = output->data.int8[kUnknownIndex] + 128;\n",
        "  uint8_t yes_score = output->data.int8[kYesIndex] + 128;\n",
        "  uint8_t no_score = output->data.int8[kNoIndex] + 128;\n",
        "  TF_LITE_MICRO_EXPECT_GT(yes_score, silence_score);\n",
        "  TF_LITE_MICRO_EXPECT_GT(yes_score, unknown_score);\n",
        "  TF_LITE_MICRO_EXPECT_GT(yes_score, no_score);\n",
        "```\n",
        "\n",
        "- 출력 텐서 데이터에서 silence, unknown, yes, no 각 클래스의 확률값을 읽음\n",
        "- yes의 확률이 다른 클래스의 확률보다 큰지 확인"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Joi3NlhOiQ7u"
      },
      "source": [
        "```cpp\n",
        "  // Now test with a different input, from a recording of \"No\".\n",
        "  const int8_t* no_features_data = g_no_micro_f9643d42_nohash_4_data;\n",
        "  for (size_t i = 0; i < input->bytes; ++i) {\n",
        "    input->data.int8[i] = no_features_data[i];\n",
        "  }\n",
        "\n",
        "  // Run the model on this \"No\" input.\n",
        "  invoke_status = interpreter.Invoke();\n",
        "  if (invoke_status != kTfLiteOk) {\n",
        "    TF_LITE_REPORT_ERROR(&micro_error_reporter, \"Invoke failed\\n\");\n",
        "  }\n",
        "  TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, invoke_status);\n",
        "\n",
        "  // Get the output from the model, and make sure it's the expected size and\n",
        "  // type.\n",
        "  output = interpreter.output(0);\n",
        "  TF_LITE_MICRO_EXPECT_EQ(2, output->dims->size);\n",
        "  TF_LITE_MICRO_EXPECT_EQ(1, output->dims->data[0]);\n",
        "  TF_LITE_MICRO_EXPECT_EQ(4, output->dims->data[1]);\n",
        "  TF_LITE_MICRO_EXPECT_EQ(kTfLiteInt8, output->type);\n",
        "\n",
        "  // Make sure that the expected \"No\" score is higher than the other classes.\n",
        "  silence_score = output->data.int8[kSilenceIndex] + 128;\n",
        "  unknown_score = output->data.int8[kUnknownIndex] + 128;\n",
        "  yes_score = output->data.int8[kYesIndex] + 128;\n",
        "  no_score = output->data.int8[kNoIndex] + 128;\n",
        "  TF_LITE_MICRO_EXPECT_GT(no_score, silence_score);\n",
        "  TF_LITE_MICRO_EXPECT_GT(no_score, unknown_score);\n",
        "  TF_LITE_MICRO_EXPECT_GT(no_score, yes_score);\n",
        "```\n",
        "\n",
        "- \"no\" 오디오 데이터에 대한 테스트 추론 및 결과 확인\n",
        "- 입력 데이터\n",
        " - micro_features/no_micro_features_data.cc에 정의된 배열인 g_no_micro_f9643d42_nohash_4_data 이용\n",
        "- 인터프리터 실행\n",
        "- 출력 텐서 확인\n",
        "- 각 클래스의 확률값을 읽고 no의 확률이 다른 것보다 큰지 확인 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNQQ3mo2jBAm"
      },
      "source": [
        "#### 테스트 실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcEinfemjMAU",
        "outputId": "893157e6-f2c3-4182-9c8a-f5ea8653bb1c"
      },
      "source": [
        "% cd /content\n",
        "% rm -rf tensorflow\n",
        "! git clone https://github.com/tensorflow/tflite-micro"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'tflite-micro'...\n",
            "remote: Enumerating objects: 5202, done.\u001b[K\n",
            "remote: Counting objects: 100% (690/690), done.\u001b[K\n",
            "remote: Compressing objects: 100% (425/425), done.\u001b[K\n",
            "remote: Total 5202 (delta 386), reused 481 (delta 256), pack-reused 4512\u001b[K\n",
            "Receiving objects: 100% (5202/5202), 8.19 MiB | 14.09 MiB/s, done.\n",
            "Resolving deltas: 100% (3523/3523), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mm89xFABjbBg"
      },
      "source": [
        "% cd /content/tflite-micro/\n",
        "\n",
        "! make -f tensorflow/lite/micro/tools/make/Makefile test_micro_speech_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAgjePSDkQgG"
      },
      "source": [
        "### 7.3.2 오디오 추출기 (audio_provider.h / audio_provider.cc)\n",
        "\n",
        "- 오디오 추출기는 장치의 마이크 하드웨어를 코드와 연결하는 역할을 수행\n",
        " - 장치마다 오디오 캡처를 위한 메커니즘이 모두 다르기 때문에 audio_provider.h는 오디오 데이터 요청을 위한 인터페이스를 정의하며 개발자는 지원하고자 하는 플랫폼에 대한 구현을 작성함\n",
        "\n",
        "- 오디오 추출기의 핵심은 GetAudioSamples() 함수\n",
        " - https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech/audio_provider.h\n",
        " - https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech/audio_provider.cc\n",
        "\n",
        "```cpp\n",
        "TfLiteStatus GetAudioSamples(tflite::ErrorReporter* error_reporter,\n",
        "                             int start_ms, int duration_ms,\n",
        "                             int* audio_samples_size, int16_t** audio_samples);\n",
        "```\n",
        "\n",
        "- 16비트 PCM(펄스 코드 변조) 오디오 데이터의 배열을 반환\n",
        "- 4개의 매개변수\n",
        " - ErrorReporter 객체\n",
        " - 시작 시간 (start_ms)\n",
        " - 기간 (duration_ms)\n",
        " - 오디오 샘플 크기 (audio_samples_size)\n",
        " - 오디오 샘플 데이터 (audio_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57bwaZUim0Rc"
      },
      "source": [
        "#### 오디오 추출기 테스트\n",
        "\n",
        "- 오디오 추출기를 사용하는 방법\n",
        " - audio_provider_test.cc에 있는 2가지 테스트 중 첫번째 테스트를 확인\n",
        "\n",
        "```cpp\n",
        "TF_LITE_MICRO_TEST(TestAudioProvider) {\n",
        "  tflite::MicroErrorReporter micro_error_reporter;\n",
        "\n",
        "  int audio_samples_size = 0;\n",
        "  int16_t* audio_samples = nullptr;\n",
        "\n",
        "  TfLiteStatus get_status =\n",
        "      GetAudioSamples(&micro_error_reporter, 0, kFeatureSliceDurationMs,\n",
        "                      &audio_samples_size, &audio_samples);\n",
        "  \n",
        "  TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, get_status);\n",
        "  TF_LITE_MICRO_EXPECT_LE(audio_samples_size, kMaxAudioSampleSize);\n",
        "  TF_LITE_MICRO_EXPECT_NE(audio_samples, nullptr);\n",
        "\n",
        "  // Make sure we can read all of the returned memory locations.\n",
        "  int total = 0;\n",
        "  for (int i = 0; i < audio_samples_size; ++i) {\n",
        "    total += audio_samples[i];\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "- GetAudioSamples 함수를 호출하기 위해 필요한 객체 및 변수를 선언하고 호출\n",
        "- 올바른 호출 결과를 얻었는지 확인\n",
        "- 반환된 audio_samples 데이터 배열에서 모든 값을 읽을 수 있는지 확인\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5pF3kI6oWai"
      },
      "source": [
        "! make -f tensorflow/lite/micro/tools/make/Makefile test_audio_provider_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI8HyxV8opwx"
      },
      "source": [
        "### 7.3.3 특징 추출기 (feature_provider.h / feature_provider.cc)\n",
        "\n",
        "- 특징 추출기는 오디오 추출기로부터 얻은 원시(raw) 오디오 데이터를 모델에 공급할 수 있는 스펙트로그램으로 변환하는 역할 수행\n",
        " - 메인 루프 중에 호출됨\n",
        "\n",
        "\n",
        "- 특징 추출기 정의\n",
        " - feature_provider.h : https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech/feature_provider.h\n",
        " - PopulateFeatureData() 함수: 현재 오디오 데이터 입력에 대한 특징 데이터(스펙트로그램)를 계산\n",
        "\n",
        "```cpp\n",
        "// Binds itself to an area of memory intended to hold the input features for an\n",
        "// audio-recognition neural network model, and fills that data area with the\n",
        "// features representing the current audio input, for example from a microphone.\n",
        "// The audio features themselves are a two-dimensional array, made up of\n",
        "// horizontal slices representing the frequencies at one point in time, stacked\n",
        "// on top of each other to form a spectrogram showing how those frequencies\n",
        "// changed over time.\n",
        "class FeatureProvider {\n",
        " public:\n",
        "  // Create the provider, and bind it to an area of memory. This memory should\n",
        "  // remain accessible for the lifetime of the provider object, since subsequent\n",
        "  // calls will fill it with feature data. The provider does no memory\n",
        "  // management of this data.\n",
        "  FeatureProvider(int feature_size, int8_t* feature_data);\n",
        "  ~FeatureProvider();\n",
        "\n",
        "  // Fills the feature data with information from audio inputs, and returns how\n",
        "  // many feature slices were updated.\n",
        "  TfLiteStatus PopulateFeatureData(tflite::ErrorReporter* error_reporter,\n",
        "                                   int32_t last_time_in_ms, int32_t time_in_ms,\n",
        "                                   int* how_many_new_slices);\n",
        "\n",
        " private:\n",
        "  int feature_size_;\n",
        "  int8_t* feature_data_;\n",
        "  // Make sure we don't try to use cached information if this is the first call\n",
        "  // into the provider.\n",
        "  bool is_first_run_;\n",
        "};\n",
        "```\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCSNADK1SESO"
      },
      "source": [
        "#### 특징 추출기 테스트\n",
        "\n",
        "- 특징 추출기 사용 방법은 feature_provider_mock_test.cc의 테스트 코드에서 확인할 수 있음\n",
        " - https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech/feature_provider_mock_test.cc\n",
        "\n",
        "- 특징 추출을 위해서는 오디오 데이터가 필요하기 때문에 모의 오디오 추출기를 사용하여 오디오 데이터를 제공하도록 설정되어 있음\n",
        " - 모의 오디오 추출기: https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech/audio_provider_mock.cc \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBIHpL5sPAPT"
      },
      "source": [
        "```cpp\n",
        "TF_LITE_MICRO_TEST(TestFeatureProviderMockYes) {\n",
        "  tflite::MicroErrorReporter micro_error_reporter;\n",
        "\n",
        "  int8_t feature_data[kFeatureElementCount];\n",
        "  FeatureProvider feature_provider(kFeatureElementCount, feature_data);\n",
        "\n",
        "  int how_many_new_slices = 0;\n",
        "  TfLiteStatus populate_status = feature_provider.PopulateFeatureData(\n",
        "      &micro_error_reporter, /* last_time_in_ms= */ 0, /* time_in_ms= */ 970,\n",
        "      &how_many_new_slices);\n",
        "  TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, populate_status);\n",
        "  TF_LITE_MICRO_EXPECT_EQ(kFeatureSliceCount, how_many_new_slices);\n",
        "\n",
        "  for (int i = 0; i < kFeatureElementCount; ++i) {\n",
        "    TF_LITE_MICRO_EXPECT_EQ(g_yes_micro_f2e59fea_nohash_1_data[i],\n",
        "                            feature_data[i]);\n",
        "  }\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DIDBDG0ovKS"
      },
      "source": [
        "```cpp\n",
        "  int8_t feature_data[kFeatureElementCount];\n",
        "  FeatureProvider feature_provider(kFeatureElementCount, feature_data);\n",
        "```\n",
        "\n",
        "- FeatureProvider 객체를 생성하기 위해 생성자를 호출하여 feature_size, feature_data 매개변수 인수 전달\n",
        " - feature_size: 스펙트로그램에 있어야 하는 데이터 원소의 전체 수. 이는 모델을 학습할 때 결정된 것이고, micro_features/micro_model_settings.h에서 kFeatureElementCount로 정의됨\n",
        " - https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech/micro_features/micro_model_settings.h\n",
        " - feature_data: 스펙트로그램 데이터를 저장할 배열에 대한 포인터\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UJZB_GcZAz0"
      },
      "source": [
        "```cpp\n",
        "  TfLiteStatus populate_status = feature_provider.PopulateFeatureData(\n",
        "      &micro_error_reporter, /* last_time_in_ms= */ 0, /* time_in_ms= */ 970,\n",
        "      &how_many_new_slices);\n",
        "```\n",
        "\n",
        "- 지난 1초 동안의 오디오 특징 데이터(스펙트로그램)을 얻기 위해 PopulateFeatureData()가 호출됨\n",
        "\n",
        "- 함수의 인수\n",
        " - MicroErrorReporter 객체\n",
        " - 함수가 마지막으로 호출된 시간을 나타내는 정수 (last_time_in_ms)\n",
        " - 현재 시간 (time_in_ms)\n",
        " - 새로운 특징 슬라이스 수(how_many_new_slices)를 계산하여 업데이트될 정수 변수에 대한 포인터. 슬라이스는 스펙트로그램의 한 행에 해당하며 일종의 시간 단위로 볼 수 있음\n",
        "\n",
        "- 항상 가장 최근(마지막 순간)의 오디오 데이터가 필요하기 때문에 특징 추출기는 마지막으로 호출된 시간(last_time_in_ms)을 현재 시간(time_in_ms)과 비교함\n",
        "\n",
        "- 마지막으로 호출된 시간과 현재 시간 사이에 캡처된 오디오에 대한 스펙트로그램 데이터를 생성하고 feature_data 배열을 업데이트하여 슬라이스를 추가한 후 1초보다 오래된 것은 삭제함\n",
        "\n",
        "- PopulateFeatureData()를 실행하면 모의 오디오 추출기에 오디오를 요청함. 모의 오디오 추출기는 yes를 나타내는 오디오 데이터를 제공하고 특징 추출기는 이를 처리하여 결과를 제공함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5DJyTyFZGiW"
      },
      "source": [
        "```cpp\n",
        "  TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, populate_status);\n",
        "  TF_LITE_MICRO_EXPECT_EQ(kFeatureSliceCount, how_many_new_slices);\n",
        "\n",
        "  for (int i = 0; i < kFeatureElementCount; ++i) {\n",
        "    TF_LITE_MICRO_EXPECT_EQ(g_yes_micro_f2e59fea_nohash_1_data[i],\n",
        "                            feature_data[i]);\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "- 추출된 데이터에 에러가 없는지 확인\n",
        "- 생성된 데이터를 모의 오디오 추출기가 제공한 yes 입력에 일치하는 스펙트로그램 데이터와 비교"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1egQgu8Zug0"
      },
      "source": [
        "```cpp\n",
        "TF_LITE_MICRO_TEST(TestFeatureProviderMockNo) {\n",
        "  tflite::MicroErrorReporter micro_error_reporter;\n",
        "\n",
        "  int8_t feature_data[kFeatureElementCount];\n",
        "  FeatureProvider feature_provider(kFeatureElementCount, feature_data);\n",
        "\n",
        "  int how_many_new_slices = 0;\n",
        "  TfLiteStatus populate_status = feature_provider.PopulateFeatureData(\n",
        "      &micro_error_reporter, /* last_time_in_ms= */ 4000,\n",
        "      /* time_in_ms= */ 4970, &how_many_new_slices);\n",
        "  TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, populate_status);\n",
        "  TF_LITE_MICRO_EXPECT_EQ(kFeatureSliceCount, how_many_new_slices);\n",
        "\n",
        "  for (int i = 0; i < kFeatureElementCount; ++i) {\n",
        "    TF_LITE_MICRO_EXPECT_EQ(g_no_micro_f9643d42_nohash_4_data[i],\n",
        "                            feature_data[i]);\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "- 두번째 테스트는 \"no\" 오디오에 대한 특징 추출기 테스트\n",
        " - last_time_in_ms와 time_in_ms 값이 달라짐"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XshD0tcnaCtO"
      },
      "source": [
        "- 테스트 실행\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70gVa5BIaL8z"
      },
      "source": [
        "% cd /content/tflite-micro/\n",
        "\n",
        "! make -f tensorflow/lite/micro/tools/make/Makefile test_feature_provider_mock_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6rbvUMUa8LG"
      },
      "source": [
        "#### 특징 추출기의 스펙트로그램 생성 방법\n",
        "\n",
        "- 특징 추출기 구현\n",
        " - feature_provider.cc : https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech/feature_provider.cc\n",
        "\n",
        "- 특징 추출기의 역할\n",
        " - 1초 오디오의 스펙트로그램을 나타내는 배열을 채우는 것\n",
        " - 루프에서 반복 호출되도록 설계됐기 때문에 현재 호출과 직전 호출 사이의 시간 동안의 데이터에 대한 특징을 생성\n",
        " - 1초가 경과되기 전에 호출된 경우 이전 출력의 일부를 유지하고 누락된 부분의 특징만 생성\n",
        "\n",
        "- 스펙트로그램은 40열 49행의 2D 배열로 표시됨\n",
        " - 각 행은 주파수 버킷 40개로 분할된 30ms 오디오 샘플의 특징을 나타냄\n",
        " - 각 행을 만들기 위해 30ms 오디오 입력 슬라이스에 FFT (Fast Fourier Transform, 고속 푸리에 변환) 알고리즘을 실행. FFT는 데이터 샘플에서 오디오 주파수 분포를 분석하고 각각 0에서 255사이 값을 갖는 256개의 주파수 버킷 배열을 생성함\n",
        " - 256개 주파수 버킷 배열은 6개 그룹으로 평균화 되어 버킷 40개를 생성함\n",
        " - 30ms 오디오 샘플 창을 20ms 간격으로 이동하며 전체 1초 샘플을 모두 처리할 때까지 위 연산을 수행\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpOHMBMqk1Je"
      },
      "source": [
        "<img src=\"./07.호출어_감지_애플리케이션/07-2.spectrogram_feature.png\" width=\"90%\" height=\"90%\">\n",
        "<center>오디오 샘플 데이터에서 스펙트로그램을 구하는 과정</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYkRlqgomY41"
      },
      "source": [
        "- feature_provider.cc의 PopulateFeatureData() 주요 코드\n",
        "\n",
        "```cpp\n",
        "  // Quantize the time into steps as long as each window stride, so we can\n",
        "  // figure out which audio data we need to fetch.\n",
        "  const int last_step = (last_time_in_ms / kFeatureSliceStrideMs);\n",
        "  const int current_step = (time_in_ms / kFeatureSliceStrideMs);\n",
        "\n",
        "  int slices_needed = current_step - last_step;\n",
        "```\n",
        "\n",
        "- PopulateFeatureData()가 마지막으로 호출된 시간을 기준으로 실제로 생성해야 하는 슬라이스를 결정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9XDxZWI1vdr"
      },
      "source": [
        "```cpp\n",
        "  // If this is the first call, make sure we don't use any cached information.\n",
        "  if (is_first_run_) {\n",
        "    TfLiteStatus init_status = InitializeMicroFeatures(error_reporter);\n",
        "    if (init_status != kTfLiteOk) {\n",
        "      return init_status;\n",
        "    }\n",
        "    is_first_run_ = false;\n",
        "    slices_needed = kFeatureSliceCount;\n",
        "  }\n",
        "  if (slices_needed > kFeatureSliceCount) {\n",
        "    slices_needed = kFeatureSliceCount;\n",
        "  }\n",
        "  *how_many_new_slices = slices_needed;\n",
        "```\n",
        "\n",
        "- 이전에 실행되지 않았거나 1초 이상 전에 실행된 경우 최대 슬라이스 수를 생성\n",
        " - 결과로 나오는 값(slices_needed)은 how_many_new_slices 변수에 기록\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaQU7of82FJl"
      },
      "source": [
        "```cpp\n",
        "  const int slices_to_keep = kFeatureSliceCount - slices_needed;\n",
        "  const int slices_to_drop = kFeatureSliceCount - slices_to_keep;\n",
        "  // If we can avoid recalculating some slices, just move the existing data\n",
        "  // up in the spectrogram, to perform something like this:\n",
        "  // last time = 80ms          current time = 120ms\n",
        "  // +-----------+             +-----------+\n",
        "  // | data@20ms |         --> | data@60ms |\n",
        "  // +-----------+       --    +-----------+\n",
        "  // | data@40ms |     --  --> | data@80ms |\n",
        "  // +-----------+   --  --    +-----------+\n",
        "  // | data@60ms | --  --      |  <empty>  |\n",
        "  // +-----------+   --        +-----------+\n",
        "  // | data@80ms | --          |  <empty>  |\n",
        "  // +-----------+             +-----------+\n",
        "  if (slices_to_keep > 0) {\n",
        "    for (int dest_slice = 0; dest_slice < slices_to_keep; ++dest_slice) {\n",
        "      int8_t* dest_slice_data =\n",
        "          feature_data_ + (dest_slice * kFeatureSliceSize);\n",
        "      const int src_slice = dest_slice + slices_to_drop;\n",
        "      const int8_t* src_slice_data =\n",
        "          feature_data_ + (src_slice * kFeatureSliceSize);\n",
        "      for (int i = 0; i < kFeatureSliceSize; ++i) {\n",
        "        dest_slice_data[i] = src_slice_data[i];\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "```\n",
        "\n",
        "- 기존 슬라이스 수를 계산하고 배열 데이터를 이동하여 새로운 슬라이스를 위한 공간을 만듦\n",
        " - slices_to_keep: 이전 슬라이스 중 유지할 슬라이스 수\n",
        " - slices_to_drop: 이전 슬라이스 중 버릴 슬라이스 수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pmz5LB_3zQj"
      },
      "source": [
        "```cpp\n",
        "    for (int new_slice = slices_to_keep; new_slice < kFeatureSliceCount;\n",
        "         ++new_slice) {\n",
        "      const int new_step = (current_step - kFeatureSliceCount + 1) + new_slice;\n",
        "      const int32_t slice_start_ms = (new_step * kFeatureSliceStrideMs);\n",
        "      int16_t* audio_samples = nullptr;\n",
        "      int audio_samples_size = 0;\n",
        "\n",
        "      // TODO(petewarden): Fix bug that leads to non-zero slice_start_ms\n",
        "      GetAudioSamples(error_reporter, (slice_start_ms > 0 ? slice_start_ms : 0),\n",
        "                      kFeatureSliceDurationMs, &audio_samples_size,\n",
        "                      &audio_samples);\n",
        "\n",
        "      if (audio_samples_size < kMaxAudioSampleSize) {\n",
        "        TF_LITE_REPORT_ERROR(error_reporter,\n",
        "                             \"Audio data size %d too small, want %d\",\n",
        "                             audio_samples_size, kMaxAudioSampleSize);\n",
        "        return kTfLiteError;\n",
        "      }\n",
        "```\n",
        "\n",
        "- 새로 만들어야 하는 슬라이스마다 한 번씩 반복되는 루프를 시작\n",
        " - GetAudioSamples() 함수를 사용하여 오디오 추출기에 해당 슬라이스에 대한 오디오 샘플을 획득\n",
        "\n",
        "\n",
        "```cpp\n",
        "      int8_t* new_slice_data = feature_data_ + (new_slice * kFeatureSliceSize);\n",
        "      size_t num_samples_read;\n",
        "\n",
        "      TfLiteStatus generate_status = GenerateMicroFeatures(\n",
        "          error_reporter, audio_samples, audio_samples_size, kFeatureSliceSize,\n",
        "          new_slice_data, &num_samples_read);\n",
        "      if (generate_status != kTfLiteOk) {\n",
        "        return generate_status;\n",
        "      }\n",
        "    }\n",
        "```\n",
        "\n",
        "- 오디오 샘플 데이터는 GenerateMicroFeatures() 함수를 이용하여 해당 슬라이스 오디오에 대한 스펙트로그램 정보를 반환\n",
        " - micro_features/micro_features_generator.h에 정의됨\n",
        " - https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech/micro_features/micro_features_generator.cc\n",
        "\n",
        "\n",
        "- 스펙트로그램 데이터가 준비되면 모델로 추론을 수행할 수 있음. 추론을 마치고 나면 결과를 해석해야 하는데 이것은 명령 인식기에서 진행됨\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTb5sp1l5gTT"
      },
      "source": [
        "### 7.3.4 명령 인식기 (recognize_commands.h / recognize_commands.cc)\n",
        "\n",
        "- RecognizeCommands 클래스\n",
        " - 모델 추론이 실행된 후 학습된 단어가 사용됐는지 추론 결과 확률 셋이 나오면 이것이 성공적인 호출어 감지를 의미하는지 여부를 판별하는 역할\n",
        " - https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech/recognize_commands.h\n",
        " - https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech/recognize_commands.cc\n",
        "\n",
        "- 입력 데이터에 대해 어떤 한 호출어 클래스의 확률이 특정 임계값 이상이면 해당 호출어가 쓰인 것으로 볼 수 있지만 실제 상황에서는 연속적인 오디오 데이터 입력에 대해 추론이 반복적으로 수행되므로 고려해야 할 점이 있음\n",
        "\n",
        "- noted를 발음하고 이를 인식하는 경우의 예"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENZorLFx5-Vs"
      },
      "source": [
        "<img src=\"./07.호출어_감지_애플리케이션/07-3.waveform.png\" width=\"60%\" height=\"60%\">\n",
        "<center>noted를 발음한 파형을 1초 윈도우로 캡처한 모습</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Igt_UKLfACIt"
      },
      "source": [
        "- 모델은 no라는 단어를 감지하도록 훈련됐으며 noted가 no와 다르다는 것은 구분할 수 있음\n",
        "\n",
        "- 위 그림에서 갈색으로 표시된 1초 윈도우에 대해서 추론을 수행하면 이를 no로 분류할 가능성은 낮을 것임\n",
        "\n",
        "- 하지만 검은색으로 표시된 1초 윈도우에 대해서 추론을 수행하면 noted의 첫 음절 부분만 해당되고 이는 no와 같기 때문에 모델은 이것이 no일 확률이 높은 것으로 해석할 수 있음\n",
        "\n",
        "- 이와 같은 문제 때문에 하나의 추론 결과에만 의존하여 호출어의 유무를 판단해서는 안 됨\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIXuuPHYBhG2"
      },
      "source": [
        "- RecognizeCommands 명령 인식기\n",
        " - 몇 개의 연속적인 추론에 대한 각 단어의 평균 점수를 계산하고 어떤 한 단어로 판단하기에 점수가 충분히 높은지 확인하는 과정을 통해 최종 결과를 냄\n",
        " - 이를 위해 추론 결과가 나올 때마다 이 결과를 인식기로 전달함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_QuE9TZFR1n"
      },
      "source": [
        "- recognize_commands.h의 RecognizeCommands 클래스 정의\n",
        "\n",
        "```cpp\n",
        "class RecognizeCommands {\n",
        " public:\n",
        "  // labels should be a list of the strings associated with each one-hot score.\n",
        "  // The window duration controls the smoothing. Longer durations will give a\n",
        "  // higher confidence that the results are correct, but may miss some commands.\n",
        "  // The detection threshold has a similar effect, with high values increasing\n",
        "  // the precision at the cost of recall. The minimum count controls how many\n",
        "  // results need to be in the averaging window before it's seen as a reliable\n",
        "  // average. This prevents erroneous results when the averaging window is\n",
        "  // initially being populated for example. The suppression argument disables\n",
        "  // further recognitions for a set time after one has been triggered, which can\n",
        "  // help reduce spurious recognitions.\n",
        "  explicit RecognizeCommands(tflite::ErrorReporter* error_reporter,\n",
        "                             int32_t average_window_duration_ms = 1000,\n",
        "                             uint8_t detection_threshold = 200,\n",
        "                             int32_t suppression_ms = 1500,\n",
        "                             int32_t minimum_count = 3);\n",
        "\n",
        "  // Call this with the results of running a model on sample data.\n",
        "  TfLiteStatus ProcessLatestResults(const TfLiteTensor* latest_results,\n",
        "                                    const int32_t current_time_ms,\n",
        "                                    const char** found_command, uint8_t* score,\n",
        "                                    bool* is_new_command);\n",
        "```\n",
        "\n",
        "- 클래스 생성자 기본값 정의\n",
        " - 평균화 창의 길이: average_window_duration_ms (1000)\n",
        " - 명령(호출어) 탐지의 기준이 되는 최소 평균 점수: detection_threshold (200)\n",
        " - 명령을 인식한 후 두 번째 명령을 인식하기 전에 기다리는 시간: suppression_ms (1500)\n",
        " - 결과를 세는 데 필요한 최소 추론 횟수: minimum_count (3)\n",
        "\n",
        "\n",
        "- ProcessLatestResults() 메소드\n",
        " - 추론 결과를 이용하여 명령(호출어)를 판단하는 역할\n",
        " - 모델 출력을 포함하는 TfLiteTensor에 대한 포인터: latest_results\n",
        " - 현재 시간: current_time_ms\n",
        " - 감지한 명령의 이름: found_command\n",
        " - 명령의 평균 점수: score\n",
        " - 명령이 이전과 다른 새로운 것인지 같은 것인지 여부: is_new_command\n",
        " - 마지막 3개는 ProcessLatestResults() 함수 호출 결과를 전달하기 위한 용도로 사용되는 포인터 변수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6wUXGWfIK6T"
      },
      "source": [
        "- ProcessLatestResults() 메소드 구현\n",
        " - https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech/recognize_commands.cc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSycHlw3JG4s"
      },
      "source": [
        "```cpp\n",
        "TfLiteStatus RecognizeCommands::ProcessLatestResults(\n",
        "    const TfLiteTensor* latest_results, const int32_t current_time_ms,\n",
        "    const char** found_command, uint8_t* score, bool* is_new_command) {\n",
        "\n",
        "  if ((latest_results->dims->size != 2) ||\n",
        "      (latest_results->dims->data[0] != 1) ||\n",
        "      (latest_results->dims->data[1] != kCategoryCount)) {\n",
        "    TF_LITE_REPORT_ERROR(\n",
        "        error_reporter_,\n",
        "        \"The results for recognition should contain %d elements, but there are \"\n",
        "        \"%d in an %d-dimensional shape\",\n",
        "        kCategoryCount, latest_results->dims->data[1],\n",
        "        latest_results->dims->size);\n",
        "    return kTfLiteError;\n",
        "  }\n",
        "\n",
        "  if (latest_results->type != kTfLiteInt8) {\n",
        "    TF_LITE_REPORT_ERROR(\n",
        "        error_reporter_,\n",
        "        \"The results for recognition should be int8_t elements, but are %d\",\n",
        "        latest_results->type);\n",
        "    return kTfLiteError;\n",
        "  }\n",
        "```\n",
        "\n",
        "- 추론 결과를 담고 있는 텐서(latest_results)가 올바른 형태와 타입을 갖고 있는지 확인"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJM-Y0GcJoSh"
      },
      "source": [
        "```cpp\n",
        "  if ((!previous_results_.empty()) &&\n",
        "      (current_time_ms < previous_results_.front().time_)) {\n",
        "    TF_LITE_REPORT_ERROR(\n",
        "        error_reporter_,\n",
        "        \"Results must be fed in increasing time order, but received a \"\n",
        "        \"timestamp of %d that was earlier than the previous one of %d\",\n",
        "        current_time_ms, previous_results_.front().time_);\n",
        "    return kTfLiteError;\n",
        "  }\n",
        "```\n",
        "\n",
        "- current_time_ms를 검사하여 평균화 창에서 가장 최근 결과를 낸 시간 이후인지 확인"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBcVOB7aJ-AK"
      },
      "source": [
        "```cpp\n",
        "  // Add the latest results to the head of the queue.\n",
        "  previous_results_.push_back({current_time_ms, latest_results->data.int8});\n",
        "\n",
        "  // Prune any earlier results that are too old for the averaging window.\n",
        "  const int64_t time_limit = current_time_ms - average_window_duration_ms_;\n",
        "  \n",
        "  while ((!previous_results_.empty()) &&\n",
        "         previous_results_.front().time_ < time_limit) {\n",
        "    previous_results_.pop_front();\n",
        "  }\n",
        "```\n",
        "\n",
        "- 최신 추론 결과를 평균화할 결과의 목록에 추가"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nHqDa3wKaCz"
      },
      "source": [
        "```cpp\n",
        "  // If there are too few results, assume the result will be unreliable and\n",
        "  // bail.\n",
        "  const int64_t how_many_results = previous_results_.size();\n",
        "  const int64_t earliest_time = previous_results_.front().time_;\n",
        "  const int64_t samples_duration = current_time_ms - earliest_time;\n",
        "  if ((how_many_results < minimum_count_) ||\n",
        "      (samples_duration < (average_window_duration_ms_ / 4))) {\n",
        "    *found_command = previous_top_label_;\n",
        "    *score = 0;\n",
        "    *is_new_command = false;\n",
        "    return kTfLiteOk;\n",
        "  }\n",
        "```\n",
        "\n",
        "- 평균화 창 내에 최소값(minimum_count 3)보다 적은 추론 결과만 있는 경우 유효한 평균을 제공할 수 없음\n",
        "- 이 경우 가장 최근의 확률이 가장 높은 명령어이며, 점수는 0이고 명령이 새로운 명령이 아닌 것으로 처리\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0zHcX_TLXU_"
      },
      "source": [
        "```cpp\n",
        "  // Calculate the average score across all the results in the window.\n",
        "  int32_t average_scores[kCategoryCount];\n",
        "\n",
        "  for (int offset = 0; offset < previous_results_.size(); ++offset) {\n",
        "    PreviousResultsQueue::Result previous_result =\n",
        "        previous_results_.from_front(offset);\n",
        "\n",
        "    const int8_t* scores = previous_result.scores;\n",
        "\n",
        "    for (int i = 0; i < kCategoryCount; ++i) {\n",
        "      if (offset == 0) {\n",
        "        average_scores[i] = scores[i] + 128;\n",
        "      } else {\n",
        "        average_scores[i] += scores[i] + 128;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "  for (int i = 0; i < kCategoryCount; ++i) {\n",
        "    average_scores[i] /= how_many_results;\n",
        "  }\n",
        "```\n",
        "\n",
        "- 각 클래스 레이블(총 4개)의 점수의 평균을 계산\n",
        " - kCategoryCount: 4 (silence, unknown, yes, no)\n",
        " - https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech/micro_features/micro_model_settings.h\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFnCXMXPNKii"
      },
      "source": [
        "```cpp\n",
        "  // Find the current highest scoring category.\n",
        "  int current_top_index = 0;\n",
        "  int32_t current_top_score = 0;\n",
        "\n",
        "  for (int i = 0; i < kCategoryCount; ++i) {\n",
        "    if (average_scores[i] > current_top_score) {\n",
        "      current_top_score = average_scores[i];\n",
        "      current_top_index = i;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  const char* current_top_label = kCategoryLabels[current_top_index];\n",
        "```\n",
        "\n",
        "- 평균 점수가 가장 높은 클래스 레이블의 인덱스를 구하고 이를 이용해 해당 레이블 값을 획득\n",
        " - kCategoryLabels[kCategoryCount] 배열\n",
        "  - https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech/micro_features/micro_model_settings.cc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTAIyZ9lON0v"
      },
      "source": [
        "```cpp\n",
        "  // If we've recently had another label trigger, assume one that occurs too\n",
        "  // soon afterwards is a bad result.\n",
        "  int64_t time_since_last_top;\n",
        "  if ((previous_top_label_ == kCategoryLabels[0]) ||\n",
        "      (previous_top_label_time_ == std::numeric_limits<int32_t>::min())) {\n",
        "    time_since_last_top = std::numeric_limits<int32_t>::max();\n",
        "  } else {\n",
        "    time_since_last_top = current_time_ms - previous_top_label_time_;\n",
        "  }\n",
        "\n",
        "  if ((current_top_score > detection_threshold_) &&\n",
        "      ((current_top_label != previous_top_label_) ||\n",
        "       (time_since_last_top > suppression_ms_))) {\n",
        "    previous_top_label_ = current_top_label;\n",
        "    previous_top_label_time_ = current_time_ms;\n",
        "    *is_new_command = true;\n",
        "  } else {\n",
        "    *is_new_command = false;\n",
        "  }\n",
        "  *found_command = current_top_label;\n",
        "  *score = current_top_score;\n",
        "```\n",
        "\n",
        "- 결과가 유효한 탐지인지 확인\n",
        " - 점수가 사전에 정의된 임계값(200)보다 높고 마지막 유효한 탐지 후에 너무 빨리 발생하지 않았는지 확인\n",
        "\n",
        "- 결과가 유효하면 is_new_command를 true로 설정\n",
        " - 함수 호출자가 실제로 명령이 감지되었는지 확인하기 위해 사용할 수 있는 변수\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQwf0tkJO9H5"
      },
      "source": [
        "#### 명령 인식기 테스트\n",
        "\n",
        "- 테스트 코드\n",
        " - https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech/recognize_commands_test.cc\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoNNg2ZgPtfV"
      },
      "source": [
        "```cpp\n",
        "TF_LITE_MICRO_TEST(RecognizeCommandsTestBasic) {\n",
        "  tflite::MicroErrorReporter micro_error_reporter;\n",
        "\n",
        "  RecognizeCommands recognize_commands(&micro_error_reporter);\n",
        "\n",
        "  const int8_t result_data[] = {127, -128, -128, -128};\n",
        "  int result_dims[] = {2, 1, 4};\n",
        "  TfLiteTensor results = tflite::testing::CreateQuantizedTensor(\n",
        "      result_data, tflite::testing::IntArrayFromInts(result_dims), -128.0f,\n",
        "      127.0f);\n",
        "\n",
        "  const char* found_command;\n",
        "  uint8_t score;\n",
        "  bool is_new_command;\n",
        "  TF_LITE_MICRO_EXPECT_EQ(\n",
        "      kTfLiteOk, recognize_commands.ProcessLatestResults(\n",
        "                     &results, 0, &found_command, &score, &is_new_command));\n",
        "}\n",
        "```\n",
        "\n",
        " - RecognizeCommands 객체 생성\n",
        " - 테스트 용으로 사용할 추론 결과를 저장한 텐서 객체 생성\n",
        " - ProcessLatestResults() 함수의 호출 결과를 저장할 변수 선언\n",
        " - ProcessLatestResults() 함수 호출\n",
        " - 함수 반환값이 kTfLiteOk인지 확인"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x7ijWKiPyZN"
      },
      "source": [
        "- 테스트 실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPuX_lSuP6GJ"
      },
      "source": [
        "% cd /content/tflite-micro/\n",
        "\n",
        "! make -f tensorflow/lite/micro/tools/make/Makefile test_recognize_commands_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuNuIs-iQvyL"
      },
      "source": [
        "### 7.3.5 명령 응답기 (command_responder.h / command_responder.cc)\n",
        "\n",
        "- 명령 응답기 역할\n",
        " - 호출어가 감지됐음을 알려주는 출력을 생성함\n",
        " - RespondToCommand() 함수\n",
        " - https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech/command_responder.h\n",
        " - https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech/command_responder.cc\n",
        "\n",
        "- 명령 응답기는 각 유형의 장치에 따라 재정의되도록 설계됨\n",
        "\n",
        "- 호출어 감지 결과를 텍스트로 기록하는 간단한 기본 구현\n",
        "\n",
        "```cpp\n",
        "// The default implementation writes out the name of the recognized command\n",
        "// to the error console. Real applications will want to take some custom\n",
        "// action instead, and should implement their own versions of this function.\n",
        "void RespondToCommand(tflite::ErrorReporter* error_reporter,\n",
        "                      int32_t current_time, const char* found_command,\n",
        "                      uint8_t score, bool is_new_command) {\n",
        "  if (is_new_command) {\n",
        "    TF_LITE_REPORT_ERROR(error_reporter, \"Heard %s (%d) @%dms\", found_command,\n",
        "                         score, current_time);\n",
        "  }\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJeJxzw_RyR3"
      },
      "source": [
        "#### 명령 응답기 테스트\n",
        "\n",
        "- 테스트 코드\n",
        " - command_responder_test.cc\n",
        " - https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech/command_responder_test.cc\n",
        "\n",
        "```cpp\n",
        "TF_LITE_MICRO_TEST(TestCallability) {\n",
        "  tflite::MicroErrorReporter micro_error_reporter;\n",
        "\n",
        "  // This will have external side-effects (like printing to the debug console\n",
        "  // or lighting an LED) that are hard to observe, so the most we can do is\n",
        "  // make sure the call doesn't crash.\n",
        "  RespondToCommand(&micro_error_reporter, 0, \"foo\", 0, true);\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrO81g1nSMb6"
      },
      "source": [
        "- 테스트 실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEmZa7RJSN4C"
      },
      "source": [
        "% cd /content/tflite-micro/\n",
        "\n",
        "! make -f tensorflow/lite/micro/tools/make/Makefile test_command_responder_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNAUXa-lSaEV"
      },
      "source": [
        "## 7.4 호출어 감지\n",
        "\n",
        "#### 주요 코드\n",
        "- main_functions.h / main_functions.cc\n",
        " - 프로그램의 핵심인 setup(), loop() 함수를 정의\n",
        " - micro_speech_test.cc에서 본 것과 유사한 형태\n",
        "\n",
        "- main.cc\n",
        " - 여기에 정의된 main() 함수에서 setup(), loop() 함수 호출\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptPYW114Y0fQ"
      },
      "source": [
        "#### 초기화 및 setup() 함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeQy0TvMUmuM"
      },
      "source": [
        "```cpp\n",
        "// Globals, used for compatibility with Arduino-style sketches.\n",
        "namespace {\n",
        "tflite::ErrorReporter* error_reporter = nullptr;\n",
        "const tflite::Model* model = nullptr;\n",
        "tflite::MicroInterpreter* interpreter = nullptr;\n",
        "TfLiteTensor* model_input = nullptr;\n",
        "FeatureProvider* feature_provider = nullptr;\n",
        "RecognizeCommands* recognizer = nullptr;\n",
        "int32_t previous_time = 0;\n",
        "\n",
        "// Create an area of memory to use for input, output, and intermediate arrays.\n",
        "// The size of this will depend on the model you're using, and may need to be\n",
        "// determined by experimentation.\n",
        "constexpr int kTensorArenaSize = 10 * 1024;\n",
        "uint8_t tensor_arena[kTensorArenaSize];\n",
        "int8_t feature_buffer[kFeatureElementCount];\n",
        "int8_t* model_input_buffer = nullptr;\n",
        "}  // namespace\n",
        "```\n",
        "\n",
        "- 전역 변수 선언\n",
        " - 모델, 인터프리터, 특징 추출기, 명령 인식기 객체 \n",
        " - 텐서 아레나, 특징 데이터를 저장할 feature_buffer 등 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxCYRG2CWlOv"
      },
      "source": [
        "```cpp\n",
        "// The name of this function is important for Arduino compatibility.\n",
        "void setup() {\n",
        "  tflite::InitializeTarget();\n",
        "\n",
        "  // Set up logging. Google style is to avoid globals or statics because of\n",
        "  // lifetime uncertainty, but since this has a trivial destructor it's okay.\n",
        "  // NOLINTNEXTLINE(runtime-global-variables)\n",
        "  static tflite::MicroErrorReporter micro_error_reporter;\n",
        "  error_reporter = &micro_error_reporter;\n",
        "\n",
        "  // Map the model into a usable data structure. This doesn't involve any\n",
        "  // copying or parsing, it's a very lightweight operation.\n",
        "  model = tflite::GetModel(g_micro_speech_model_data);\n",
        "  if (model->version() != TFLITE_SCHEMA_VERSION) {\n",
        "    TF_LITE_REPORT_ERROR(error_reporter,\n",
        "                         \"Model provided is schema version %d not equal \"\n",
        "                         \"to supported version %d.\",\n",
        "                         model->version(), TFLITE_SCHEMA_VERSION);\n",
        "    return;\n",
        "  }\n",
        "\n",
        "  // Pull in only the operation implementations we need.\n",
        "  // This relies on a complete list of all the ops needed by this graph.\n",
        "  // An easier approach is to just use the AllOpsResolver, but this will\n",
        "  // incur some penalty in code space for op implementations that are not\n",
        "  // needed by this graph.\n",
        "  //\n",
        "  // tflite::AllOpsResolver resolver;\n",
        "  // NOLINTNEXTLINE(runtime-global-variables)\n",
        "  static tflite::MicroMutableOpResolver<4> micro_op_resolver(error_reporter);\n",
        "  if (micro_op_resolver.AddDepthwiseConv2D() != kTfLiteOk) {\n",
        "    return;\n",
        "  }\n",
        "  if (micro_op_resolver.AddFullyConnected() != kTfLiteOk) {\n",
        "    return;\n",
        "  }\n",
        "  if (micro_op_resolver.AddSoftmax() != kTfLiteOk) {\n",
        "    return;\n",
        "  }\n",
        "  if (micro_op_resolver.AddReshape() != kTfLiteOk) {\n",
        "    return;\n",
        "  }\n",
        "\n",
        "  // Build an interpreter to run the model with.\n",
        "  static tflite::MicroInterpreter static_interpreter(\n",
        "      model, micro_op_resolver, tensor_arena, kTensorArenaSize, error_reporter);\n",
        "  interpreter = &static_interpreter;\n",
        "\n",
        "  // Allocate memory from the tensor_arena for the model's tensors.\n",
        "  TfLiteStatus allocate_status = interpreter->AllocateTensors();\n",
        "  if (allocate_status != kTfLiteOk) {\n",
        "    TF_LITE_REPORT_ERROR(error_reporter, \"AllocateTensors() failed\");\n",
        "    return;\n",
        "  }\n",
        "```\n",
        "\n",
        "- MicroErrorReporter 객체 생성\n",
        "- Model 객체 생성\n",
        "- 모델 실행에 필요한 연산을 로드하기 위해 MicroMutableOpResolver 객체 생성\n",
        " - 2D conv\n",
        " - Fully connected\n",
        " - Softmax\n",
        " - Reshape\n",
        "- MicroInterpreter 객체 생성\n",
        "- 모델 실행을 위해 사용할 텐서 메모리 할당"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKU8mTvlXsBM"
      },
      "source": [
        "```cpp\n",
        "  // Get information about the memory area to use for the model's input.\n",
        "  model_input = interpreter->input(0);\n",
        "\n",
        "  if ((model_input->dims->size != 2) || (model_input->dims->data[0] != 1) ||\n",
        "      (model_input->dims->data[1] !=\n",
        "       (kFeatureSliceCount * kFeatureSliceSize)) ||\n",
        "      (model_input->type != kTfLiteInt8)) {\n",
        "    TF_LITE_REPORT_ERROR(error_reporter,\n",
        "                         \"Bad input tensor parameters in model\");\n",
        "    return;\n",
        "  }\n",
        "\n",
        "  model_input_buffer = model_input->data.int8;\n",
        "```\n",
        "\n",
        "- 입력 텐서의 형태와 타입이 올바른지 확인"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMy_dWhaYCTx"
      },
      "source": [
        "```cpp\n",
        "  // Prepare to access the audio spectrograms from a microphone or other source\n",
        "  // that will provide the inputs to the neural network.\n",
        "  // NOLINTNEXTLINE(runtime-global-variables)\n",
        "  static FeatureProvider static_feature_provider(kFeatureElementCount,\n",
        "                                                 feature_buffer);\n",
        "  feature_provider = &static_feature_provider;\n",
        "\n",
        "  static RecognizeCommands static_recognizer(error_reporter);\n",
        "  recognizer = &static_recognizer;\n",
        "\n",
        "  previous_time = 0;\n",
        "```\n",
        "\n",
        "- 특징 추출기 (FeatureProvider) 객체 생성\n",
        "- 명령 인식기 (RecognizeCommands) 객체 생성\n",
        "- 시작 시간 초기화 (previous_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ucjw2X0YrEQ"
      },
      "source": [
        "#### loop() 함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPx5Gz6KY9K2"
      },
      "source": [
        "```cpp\n",
        "// The name of this function is important for Arduino compatibility.\n",
        "void loop() {\n",
        "  // Fetch the spectrogram for the current time.\n",
        "  const int32_t current_time = LatestAudioTimestamp();\n",
        "  int how_many_new_slices = 0;\n",
        "\n",
        "  TfLiteStatus feature_status = feature_provider->PopulateFeatureData(\n",
        "      error_reporter, previous_time, current_time, &how_many_new_slices);\n",
        "  if (feature_status != kTfLiteOk) {\n",
        "    TF_LITE_REPORT_ERROR(error_reporter, \"Feature generation failed\");\n",
        "    return;\n",
        "  }\n",
        "  previous_time = current_time;\n",
        "\n",
        "  // If no new audio samples have been received since last time, don't bother\n",
        "  // running the network model.\n",
        "  if (how_many_new_slices == 0) {\n",
        "    return;\n",
        "  }\n",
        "```\n",
        "\n",
        "- 특징 추출기를 이용하여 스펙트로그램 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C12yoXLPZZx6"
      },
      "source": [
        "```cpp\n",
        "  // Copy feature buffer to input tensor\n",
        "  for (int i = 0; i < kFeatureElementCount; i++) {\n",
        "    model_input_buffer[i] = feature_buffer[i];\n",
        "  }\n",
        "```\n",
        "\n",
        "- 추출된 특징 데이터(스펙트로그램)를 저장한 feature_buffer 배열의 값을 입력 텐서에 복사"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzdEFN_GZn_0"
      },
      "source": [
        "```cpp\n",
        "  // Run the model on the spectrogram input and make sure it succeeds.\n",
        "  TfLiteStatus invoke_status = interpreter->Invoke();\n",
        "  if (invoke_status != kTfLiteOk) {\n",
        "    TF_LITE_REPORT_ERROR(error_reporter, \"Invoke failed\");\n",
        "    return;\n",
        "  }\n",
        "```\n",
        "\n",
        "- 인터프리터를 실행하여 모델의 출력 결과를 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA1RGgyPZv5i"
      },
      "source": [
        "```cpp\n",
        "  // Obtain a pointer to the output tensor\n",
        "  TfLiteTensor* output = interpreter->output(0);\n",
        "\n",
        "  // Determine whether a command was recognized based on the output of inference\n",
        "  const char* found_command = nullptr;\n",
        "  uint8_t score = 0;\n",
        "  bool is_new_command = false;\n",
        "\n",
        "  TfLiteStatus process_status = recognizer->ProcessLatestResults(\n",
        "      output, current_time, &found_command, &score, &is_new_command);\n",
        "      \n",
        "  if (process_status != kTfLiteOk) {\n",
        "    TF_LITE_REPORT_ERROR(error_reporter,\n",
        "                         \"RecognizeCommands::ProcessLatestResults() failed\");\n",
        "    return;\n",
        "  }\n",
        "```\n",
        "\n",
        "- 모델의 출력 텐서 데이터를 이용하여 호출어 감지 수행\n",
        " - 모델 출력 텐서에는 각 클래스 레이블의 확률이 기록됨\n",
        " - 이를 ProcessLatestResults() 함수로 전달하여 처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-Pt0j6halBa"
      },
      "source": [
        "```cpp\n",
        "  // Do something based on the recognized command. The default implementation\n",
        "  // just prints to the error console, but you should replace this with your\n",
        "  // own function for a real application.\n",
        "  RespondToCommand(error_reporter, current_time, found_command, score,\n",
        "                   is_new_command);\n",
        "```\n",
        "\n",
        "- 명령 응답기의 RespondToCommand() 메소드를 호출하여 호출어 감지 결과를 출력"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ejeaygHbBVR"
      },
      "source": [
        "#### 애플리케이션 실행\n",
        "\n",
        "- 애플리케이션 빌드 수행\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS9oMfa3bJ73"
      },
      "source": [
        "! make -f tensorflow/lite/micro/tools/make/Makefile micro_speech"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNy57P-ybWBT"
      },
      "source": [
        "- 빌드된 애플리케이션 실행 파일 위치 (colab에서 빌드한 경우)\n",
        " - tensorflow/lite/micro/tools/make/gen/linux_x86_64_default/bin/micro_speech\n",
        "\n",
        " - colab에서는 오디오 데이터를 추출할 수 있는 환경이 안 되기 때문에 실행하여 결과를 확인해 볼 수는 없음\n",
        "\n",
        "- 출력 예제\n",
        "\n",
        "Heard yes(201) @4056ms\n",
        "\n",
        "\n",
        "Heard no(205) @6448ms\n",
        "\n",
        "\n",
        "Heard unknown(201) @13696ms\n",
        "\n",
        "\n",
        "Heard yes(205) @15000ms\n",
        "\n",
        "\n",
        " - 감지된 단어\n",
        " - 점수 (괄호 안): 명령 인식기는 점수가 200보다 큰 경우에만 일치하는 것으로 간주하므로 모두 200 이상 \n",
        " - 프로그램이 시작된 이후 경과한 시간 (밀리초)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYW7XZtSfylw"
      },
      "source": [
        "## 7.5 마이크로컨트롤러에 배포하기\n",
        "\n",
        "- 장치별 버전\n",
        " - 모든 장치에는 자체적인 오디오 캡처 메커니즘이 있으므로 각 장치마다 audio_provider.cc를 별도로 구현해야 함\n",
        " - 출력도 마찬가지므로 command_responder.cc도 장치별 버전이 필요함\n",
        "\n",
        "- 아두이노용 코드 저장소\n",
        " - https://github.com/tensorflow/tflite-micro-arduino-examples/tree/main/examples/micro_speech\n",
        "\n",
        " - audio_provider.cc의 아두이노 버전: arduino_audio_provider.cpp\n",
        " - command_responder.cc의 아두이노 버전: arduino_command_responder.cpp\n",
        "\n",
        " - 아두이노 나노 33 BLE 센스를 기준으로 구현된 코드이고 만약 다른 아두이노 보드를 사용하고 별도의 마이크를 연결하려면 audio_provider.cc를 직접 바꿔서 구현해야 함\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_GgDvnxh355"
      },
      "source": [
        "#### 아두이노의 명령 반응\n",
        "\n",
        "- 인식된 결과에 따라 LED를 약 3초간 켬\n",
        " - yes가 인식되면 녹색\n",
        " - no가 인식되면 빨간색 \n",
        " - unknown인 경우 파란색\n",
        "\n",
        "- arduino_command_responder.cpp 주요 코드\n",
        " - https://github.com/tensorflow/tflite-micro-arduino-examples/blob/main/examples/micro_speech/arduino_command_responder.cpp\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCV1Au3vjtFS"
      },
      "source": [
        "```cpp\n",
        "// Toggles the built-in LED every inference, and lights a colored LED depending\n",
        "// on which word was detected.\n",
        "void RespondToCommand(tflite::ErrorReporter* error_reporter,\n",
        "                      int32_t current_time, const char* found_command,\n",
        "                      uint8_t score, bool is_new_command) {\n",
        "\n",
        "  static bool is_initialized = false;\n",
        "\n",
        "  if (!is_initialized) {\n",
        "    pinMode(LED_BUILTIN, OUTPUT);\n",
        "    // Pins for the built-in RGB LEDs on the Arduino Nano 33 BLE Sense\n",
        "    pinMode(LEDR, OUTPUT);\n",
        "    pinMode(LEDG, OUTPUT);\n",
        "    pinMode(LEDB, OUTPUT);\n",
        "    // Ensure the LED is off by default.\n",
        "    // Note: The RGB LEDs on the Arduino Nano 33 BLE\n",
        "    // Sense are on when the pin is LOW, off when HIGH.\n",
        "    digitalWrite(LEDR, HIGH);\n",
        "    digitalWrite(LEDG, HIGH);\n",
        "    digitalWrite(LEDB, HIGH);\n",
        "    is_initialized = true;\n",
        "  }\n",
        "  static int32_t last_command_time = 0;\n",
        "  static int count = 0;\n",
        "  static int certainty = 220;\n",
        "```\n",
        "\n",
        "- 내장 LED 핀을 출력 모드로 설정\n",
        " - is_initialized라는 static bool 변수를 이용해 한 번만 실행되는 if 문 내에서 작업 수행\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__ELnBdZkM8I"
      },
      "source": [
        "```cpp\n",
        "  if (is_new_command) {\n",
        "    TF_LITE_REPORT_ERROR(error_reporter, \"Heard %s (%d) @%dms\", found_command,\n",
        "                         score, current_time);\n",
        "    // If we hear a command, light up the appropriate LED\n",
        "    if (found_command[0] == 'y') {\n",
        "      last_command_time = current_time;\n",
        "      digitalWrite(LEDG, LOW);  // Green for yes\n",
        "    }\n",
        "\n",
        "    if (found_command[0] == 'n') {\n",
        "      last_command_time = current_time;\n",
        "      digitalWrite(LEDR, LOW);  // Red for no\n",
        "    }\n",
        "\n",
        "    if (found_command[0] == 'u') {\n",
        "      last_command_time = current_time;\n",
        "      digitalWrite(LEDB, LOW);  // Blue for unknown\n",
        "    }\n",
        "  }\n",
        "```\n",
        "\n",
        "- is_new_command 변수가 true이면 새로운 호출어를 인식한 것이므로 error_reporter 객체를 이용해 결과 출력\n",
        "- 인식 결과를 저장한 found_command 문자 배열의 첫 문자를 확인하여 \"yes\", \"no\", \"unknown\" 결과에 따라 해당 LED를 켬"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsPxHRIyk2dS"
      },
      "source": [
        "```cpp\n",
        "  // If last_command_time is non-zero but was >3 seconds ago, zero it\n",
        "  // and switch off the LED.\n",
        "  if (last_command_time != 0) {\n",
        "    if (last_command_time < (current_time - 3000)) {\n",
        "      last_command_time = 0;\n",
        "      digitalWrite(LED_BUILTIN, LOW);\n",
        "      digitalWrite(LEDR, HIGH);\n",
        "      digitalWrite(LEDG, HIGH);\n",
        "      digitalWrite(LEDB, HIGH);\n",
        "    }\n",
        "    // If it is non-zero but <3 seconds ago, do nothing.\n",
        "    return;\n",
        "  }\n",
        "```\n",
        "\n",
        "- 3초 후에 LED를 끄는 동작을 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hja4mdLqlb6X"
      },
      "source": [
        "```cpp\n",
        "  // Otherwise, toggle the LED every time an inference is performed.\n",
        "  ++count;\n",
        "  if (count & 1) {\n",
        "    digitalWrite(LED_BUILTIN, HIGH);\n",
        "  } else {\n",
        "    digitalWrite(LED_BUILTIN, LOW);\n",
        "  }\n",
        "```\n",
        "\n",
        "- 추론이 수행될 때마다 BUILTIN LED (주황색 LED) 점등\n",
        " - 추론 횟수를 저장하는 count 변수\n",
        " - count 변수와 1에 대해 AND 연산 수행\n",
        " - count가 홀수이면 AND 결과는 1, 짝수이면 AND 결과는 0이 됨\n",
        " - 홀수일 때 LED를 끄고 짝수일 때 LED를 켬"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIfxgdNEoJCd"
      },
      "source": [
        "#### 아두이노에서 예제 실행\n",
        "\n",
        "- 예제 로드\n",
        " - 파일 메뉴 >> 예제 >> Arduino_TensorFlowLite >> micro_speech 선택\n",
        " - 첫번째 열리는 탭인 micro_speech 파일이 main_functions.cc에 해당\n",
        "\n",
        "- USB 케이블로 아두이노를 컴퓨터에 연결\n",
        "\n",
        "- Arduino IDE에서 해당 보드(Arduino Nano 33 BLE) 선택\n",
        " - 툴 메뉴 >> 보드\n",
        "\n",
        "- Arduino IDE에서 업로드 버튼(오른쪽 방향 화살표) 클릭\n",
        "\n",
        "- \"yes\", \"no\"라고 말하고 그것이 인식되면 LED가 서로 다른 빛으로 점등\n",
        " - 천천히 말해야 잘 인식되는 것으로 보임"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yj6rPfSpoCE"
      },
      "source": [
        "## 7.6 실습 과제\n",
        "\n",
        "- 아두이노 IDE에서 소스 파일을 편집하여 커스텀 버전 애플리케이션을 만들어서 실행해보자\n",
        "\n",
        " - yes라고 할 때만 파란색 LED가 켜지고 다른 경우는 안 켜지게 만든다\n",
        "\n",
        " - 애플리케이션이 모스 부호처럼 yes와 no의 특정 순서 조합에 응답하도록 만든다. 예를 들어 yes no yes가 인식되면 녹색 LED를 켜고, no no no가 인식되면 빨간 LED를 켬\n",
        " "
      ]
    }
  ]
}